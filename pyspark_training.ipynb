{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3SN0jd2pAml"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mydata = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"original.csv\")\n",
        "mydata.show()\n"
      ],
      "metadata": {
        "id": "KNlYZHYSqu8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **spark.read.format(\"csv\")**\n",
        "\n",
        "Sparks read csv format file\n",
        "\n",
        "## **option(\"header\",\"true\")**\n",
        "\n",
        "Header = true allows csv file first line as header\n",
        "\n",
        "## **load(\"original.csv\")**\n",
        "\n",
        "origional.csv is the file which contains data\n",
        "\n",
        "## **mydata**\n",
        "Values or data stored in to mydata, and mydata is a dataframe\n",
        "\n",
        "## **What is a DataFrame?**\n",
        "A DataFrame is a data structure that organizes data into a 2-dimensional table of rows and columns, much like a spreadsheet.\n",
        "Every DataFrame contains a blueprint, known as a schema, that defines the name and data type of each column. Missing or incomplete values are stored as **null** values in the DataFrame.\n"
      ],
      "metadata": {
        "id": "Wwa_Sw52VH_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "mydata2 = mydata.withColumn(\"clean_city\", when(mydata.City.isNull(), 'Unknown').otherwise(mydata.City))\n",
        "mydata2.show()"
      ],
      "metadata": {
        "id": "37kan7hYsBv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# from pyspark.sql.functions import *\n",
        "imports pyspark.sql.functions\n",
        "\n",
        "mydata.withColumn(\"clean_city\", when(mydata.City.isNull(), 'Unknown').otherwise(mydata.City))\n",
        "\n",
        "**when(mydata.City.isNull(), 'Unknown')**\n",
        "\n",
        "When City is null, replace with Unknown\n",
        "\n",
        "**.otherwise(mydata.City))**\n",
        "\n",
        "otherwise use data from City itself\n",
        "\n",
        "**mydata.withColumn(\"clean_city\",**\n",
        "\n",
        "Add Unknown values  into clen_city a new column"
      ],
      "metadata": {
        "id": "q5EDqS_4W_69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata2 = mydata2.filter(mydata2.JobTitle.isNotNull())\n",
        "mydata2.show()"
      ],
      "metadata": {
        "id": "fUz9Lj9r1lcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **mydata2.filter(mydata2.JobTitle.isNotNull()**\n",
        "\n",
        "Filter if JobTitle is Not Null, show me rows with value, dont show me empty rows"
      ],
      "metadata": {
        "id": "1lbe1TxmiGDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata2 = mydata2.withColumn(\"clean_salary\", mydata2.Salary.substr(2,100).cast('float'))\n",
        "mydata2.show()"
      ],
      "metadata": {
        "id": "kSjUN05z2PyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **mydata2.Salary.substr(2,100).cast('float'))**\n",
        "\n",
        "Subtring removes $ sign creates new column clean_salary and typecast from string to float"
      ],
      "metadata": {
        "id": "fYEriLEzi5ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "mean = mydata2.groupBy().avg('clean_salary')\n",
        "mean.show()\n"
      ],
      "metadata": {
        "id": "iOFz12Lf3Add"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "groupBy is similar to SQL command, avg() function to get the average of clean_salary"
      ],
      "metadata": {
        "id": "6uu1n_lmkK_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = mydata2.groupby().avg('clean_salary').take(1)[0][0]\n",
        "mydata2.show()"
      ],
      "metadata": {
        "id": "6U8l3H97np7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## when clean_salary isNull, USE mean VALUE otherwise LEAVE clean_salar DATA\n"
      ],
      "metadata": {
        "id": "jgLJ8zwloYr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata2 = mydata2.withColumn('new_salary', when(mydata2.clean_salary.isNull(), lit(mean)).otherwise(mydata2.clean_salary))\n",
        "mydata2.show()"
      ],
      "metadata": {
        "id": "mrlSNIfY4VXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SELECT ONE ITEM"
      ],
      "metadata": {
        "id": "uDaBOanFo2Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "latitudes = mydata2.select('Latitude')\n",
        "latitudes.show()"
      ],
      "metadata": {
        "id": "SBvVs_E4m4ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## filter Latitude isNotNull"
      ],
      "metadata": {
        "id": "kLit1OTLo9c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latitudes = latitudes.filter(latitudes.Latitude.isNotNull())\n",
        "latitudes.show()"
      ],
      "metadata": {
        "id": "CE3xnD-Noe2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## withColumn latitude2  Latitude cast TO float"
      ],
      "metadata": {
        "id": "4MPfKPrUpMOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latitudes =latitudes.withColumn('latitude2', latitudes.Latitude.cast('float')).select('latitude2')\n",
        "latitudes.show()"
      ],
      "metadata": {
        "id": "jAiljLDpo8su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FIND MEDIAN"
      ],
      "metadata": {
        "id": "w89e6crppp3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "median = np.median(latitudes.collect())\n",
        "print(median)"
      ],
      "metadata": {
        "id": "xFG9C0xrqUoN",
        "outputId": "c02bd65d-1092-4ebb-bf62-7408937cd994",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31.93397331237793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## withColumn lat  when Latitude isNull USE median otherwise USE Latitude"
      ],
      "metadata": {
        "id": "Bx07g-HdpxOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata2 = mydata2.withColumn('lat', when(mydata2.Latitude.isNull(), lit(median)).otherwise(mydata2.Latitude))\n",
        "mydata2.show()"
      ],
      "metadata": {
        "id": "2GY1m6aVqmKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## groupby gender\n",
        " ## agg avg('new_salary')\n",
        " ## alias('AvgSalary'))\n"
      ],
      "metadata": {
        "id": "57LiWTuorE1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as sqlfunc\n",
        "genders = mydata2.groupby('gender').agg(sqlfunc.avg('new_salary').alias('AvgSalary'))\n",
        "genders.show()"
      ],
      "metadata": {
        "id": "sZjNsz8OrRPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "withColumn female_salary, male_salary when( gender == 'Female',Male USE  new_salary)\n",
        ".otherwise(lit(0)))"
      ],
      "metadata": {
        "id": "VvH9oHH8r3FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = mydata2.withColumn('female_salary', when(mydata2.gender == 'Female', mydata2.new_salary).otherwise(lit(0)))\n",
        "df = df.withColumn('male_salary', when(mydata2.gender == 'Male', mydata2.new_salary).otherwise(lit(0)))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "3wS0YWQIsToX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## groupBy('JobTitle').\n",
        "##agg(\n",
        "## sqlfunc.avg('female_salary').alias('final_female_salary'),\n",
        "## sqlfunc.avg('male_salary').alias('final_male_salary'))\n"
      ],
      "metadata": {
        "id": "mpFpypQTspHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.groupBy('JobTitle').agg(sqlfunc.avg('female_salary').alias('final_female_salary'), sqlfunc.avg('male_salary').alias('final_male_salary'))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "eXriY6HkE9XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## final_female_salary - final_male_salary)\n"
      ],
      "metadata": {
        "id": "GR3xSzoutUyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('delta', df.final_female_salary - df.final_male_salary)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "KmbwS95GFlqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##groupBy('City').\n",
        "\n",
        "##agg(sqlfunc.avg('new_salary').\n",
        "\n",
        "##alias('avg_salary'))\n"
      ],
      "metadata": {
        "id": "wopgomlDtfJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cityavg = mydata2.groupBy('City').agg(sqlfunc.avg('new_salary').alias('avg_salary'))\n",
        "\n",
        "cityavg.show()"
      ],
      "metadata": {
        "id": "N4Vh4e2pH8ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##cityavg sort(col('avg_salary').desc())"
      ],
      "metadata": {
        "id": "CPFfzUQ2tydQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cityavg = cityavg.sort(col('avg_salary').desc())\n",
        "cityavg.show()"
      ],
      "metadata": {
        "id": "J3bHNhMMIZab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jLeHuOQ6JEWh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}